# 1.为什么大模型上线需要做推理优化？
计算墙和内存墙。
（1）缩短推理延迟，追求最好的用户体验；（2）减少GPU显存占用，降低部署成本
# 2.大模型中的推理优化技术，你能聊聊吗？
## 
（1）分布式优化：张量并行，流水线并行，NCCL通信优化，重点是通信层面的优化；NCCL（NVIDIA Collective Communications Library）是由 NVIDIA 开发的一种高性能的多 GPU 通信库，用于在多个 NVIDIA GPU 之间实现快速的数据传输和协同计算。它可以在深度学习和高性能计算领域中提供分布式训练和数据并行加速的支持。
## 
（2）低比特量化：INT4/INT8 权重量化，AWQ自适应量化，KV Cache量化，目的是降低显存占用，缩放因子，偏移量，量化（推理前）->反量化（推理时）->量化（推理后）。

大模型的量化是一种优化技术，其过程包括将深度学习模型中的权重和激活值从高精度浮点数（如32位）转换为低精度表示（如8位整数，INT8），这个过程被称为“量化”。它旨在减少模型的大小和计算复杂性，同时尽可能减少精度损失的优化手段。

量化方法：
后训练量化（Post-Training Quantization, PTQ）：在模型训练完成后，将浮点数参数转换为低精度整数。这种方法简单直接，但在某些情况下可能会导致精度下降。
量化感知训练（Quantization-Aware Training, QAT）：在训练过程中考虑量化的影响，通过模拟低精度计算来调整模型参数，从而在量化后保持较高的精度。

INT8量化是将模型的权重和激活值从浮点数转换为8位整数的过程。虽然INT8整数表示的数值范围较小，精度较低，但它可以显著减少存储和计算的需求。在INT8量化中，模型的权重和激活值会经过一个量化过程，包括缩放和偏移，以尽可能保留原始浮点数的信息。在推理时，这些量化值会被反量化回浮点数进行计算，然后再量化回INT8进行下一步操作。这种方法在保持较高计算效率的同时，能够在大多数应用中提供足够的精度；

AWQ（Activation-aware Weight Quantization）是一种用于大型语言模型（LLM）压缩和加速的技术。它通过自适应地选择每个权重的量化精度来优化模型性能和压缩效率的平衡。具体来说，AWQ会分析模型的每个权重，根据其重要性动态调整量化精度。重要的权重会被赋予更高的精度，而不那么重要的权重则使用更低的精度。这种方法不仅能显著减少模型的总体大小，还能保持模型的性能。
## 
（3）算子优化：算子融合，GEMM高性能算子，提升CUDA算子的计算效率，；
## 
（4）访存优化（显存优化）：PagedAttention，FlashAttention，减少GPU对HBM的访问以及提升显存利用率，高频宽存储器（HBM,High Bandwidth Memory），是将DRAM透过先进封装技术堆叠而成，与GPU整合于同一块芯片上，更有利于就近存取、传输资料；
> PagedAttention的工作原理：PagedAttention的核心思想是通过优化显存管理来提高大模型推理的效率。<br>
> 具体来说，它使用了以下几种技术：<br>
>  显存分页管理：PagedAttention将KV（Key-Value）张量存储在非连续的显存空间中，这样可以更灵活地管理显存。这种方法类似于操作系统中的分页管理，可以有效减少显存碎片，提高显存利用率。<br>
>  Copy-on-write：在自回归生成过程中，PagedAttention使用Copy-on-write技术来避免不必要的数据复制。这意味着只有在需要修改数据时才会进行实际的复制操作，从而节省了显存和计算资源。<br>
>  动态内存分配：PagedAttention可以根据实际需要动态分配内存，这样可以更好地应对不同输入长度和批次大小带来的内存需求变化。<br>

> PagedAttention的优势：<br>
> 显存效率：通过优化显存管理，PagedAttention可以在不增加显存占用的情况下处理更大的模型和更长的输入序列。<br>
> 计算效率：Copy-on-write和动态内存分配技术可以减少不必要的计算和数据复制，从而提高推理速度。<br>
> 灵活性：PagedAttention可以适应不同的硬件环境和任务需求，具有很高的灵活性。

>FlashAttention的实现原理<br>
分块计算（Tiling）： FlashAttention通过将QKV（Query, Key, Value）矩阵分块处理，减少了对内存的访问次数1。这种方法将注意力计算分解为多个小块，每个小块在GPU的片上存储器（SRAM）中进行计算，从而减少了对高带宽内存（HBM）的访问2。<br>
IO感知（IO-Awareness）： FlashAttention是一种IO感知的精确注意力算法。它通过优化内存读写操作，减少了GPU高带宽内存和片上存储器之间的数据传输2。这种方法通过减少内存访问次数，提高了计算效率。<br>
并行化处理： FlashAttention通过并行化处理来提高计算速度。它在前向传播过程中，将注意力操作融合到一个GPU内核中，从而消除了中间读写操作2。这种方法不仅提高了计算速度，还减少了内存占用。<br>
低精度计算： FlashAttention-3引入了低精度计算（如FP8），进一步提高了计算效率3。这种方法利用硬件对低精度计算的支持，在保持计算精度的同时，显著提高了计算速度。<br>

> FlashAttention的优势<br>
高效内存使用：通过减少内存访问次数和优化内存管理，FlashAttention显著提高了内存使用效率。<br>
快速计算：分块计算和并行化处理使得FlashAttention在处理大规模数据时具有显著的速度优势。<br>
灵活性：支持低精度计算，使其在不同硬件环境下都能高效运行。 
## 
（5）服务并发优化：Continous Batching，Dynamic Batching，Asyns Serving；
> Continuous Batching
Continuous Batching（连续批处理）是一种优化大规模语言模型（LLM）推理效率的方法。它通过动态调整批处理大小来提高GPU利用率。在传统的静态批处理中，批处理大小在整个推理过程中保持不变，这可能导致GPU资源未被充分利用。Continuous Batching通过在每次迭代中动态调整批处理大小，使得一旦某个序列完成生成，就可以立即插入新的序列，从而提高GPU的利用率和吞吐量。

> Dynamic Batching
Dynamic Batching（动态批处理）与Continuous Batching类似，但它更强调在不同请求之间动态调整批处理大小。Dynamic Batching可以根据实际的请求负载和模型的计算需求，动态地将多个请求合并成一个批次进行处理。这种方法可以显著减少推理延迟，提高系统的吞吐量。

> Asynchronous Serving
Asynchronous Serving（异步服务）是一种通过异步处理请求来提高系统响应速度和吞吐量的方法。在异步服务中，请求被立即接收并排队处理，而不是等待当前请求完成后再处理下一个请求。这种方法可以有效减少请求的等待时间，提高系统的并发处理能力。

> 应用场景
Continuous Batching：适用于需要高效处理长序列数据的任务，如自然语言生成和对话系统。
Dynamic Batching：适用于需要处理大量并发请求的场景，如在线推理服务和实时数据处理。
Asynchronous Serving：适用于需要高并发和低延迟的应用，如实时推荐系统和在线搜索引擎。
## 
（6）其他
